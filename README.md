# Converting nfdump files to Parquet or Clickhouse

Tooling to convert [nfcapd](https://github.com/phaag/nfdump) files to parquet format.
By default the output is 'hive partitioned' per date and hour, as shown below.
The date and hour are taken from the 'te' field (ending time) of each flow record.

```

date=2023-02-25
|-- hour=00
|   |-- 202302250000.parquet
|   |-- 202302250005.parquet
|   |-- 202302250010.parquet
|   |-- 202302250015.parquet
|   |-- 202302250020.parquet
|   |-- 202302250025.parquet
|   |-- 202302250030.parquet
|   |-- 202302250035.parquet
|   |-- 202302250040.parquet
|   |-- 202302250045.parquet
|   |-- 202302250050.parquet
|   `-- 202302250055.parquet
|-- hour=01
|   |-- 202302250100.parquet
|   |-- 202302250105.parquet
|   |-- 202302250110.parquet
|   |-- 202302250115.parquet
|   |-- 202302250120.parquet
|   |-- 202302250125.parquet
|   |-- 202302250130.parquet
|   |-- 202302250135.parquet
|   |-- 202302250140.parquet
|   |-- 202302250145.parquet
|   |-- 202302250150.parquet
|   `-- 202302250155.parquet
    |-- hour=02
(...)
```

Hives partitioning can be disabled with a -n or --nohives argument to either watch or nfdump2parquet, 
in which case the date and hour are stored in the parquet file(s) itself.

### nfdump2parquet
nfdump2parquet does the actual conversion from nfcapd file to parquet. 
It works by calling nfdump to convert an nfcapd file to a temporary CSV file (without headers or summary):
`nfdump -r nfcapd.202211070100 -o CSV -q`
The CSV is loaded with [pyarrow](https://arrow.apache.org/docs/python/index.html), 
the date and hour columns are added (derived from each flow record) and the result is then 
written to the specified output directory (with hives partitioning by default).
CSV files are loaded in chunks of 2GB, the maximum possible for pyarrow.

Fired from the command line it can convert a single file, but if the source argument is a directory then all
nfcapd files in that directory will be converted one by one. Specifying -r or --recursive will make it search all 
subdirectories (recursively) for nfcapd files as well.

By default the following fields from the netflow records will be stored in the parquet file:
ts, te, td, sa, da, sp, dp, pr, flg, ipkt, ibyt, opkt, obyt.
I will probably make this configurable at some point in the future 
(either via command line arguments or possibly a config file), for now you can change this manually in the
nfdump2parquet.py file (*parquet_fields* variable at the top of the file, in the class definition of *Nfdump2Parquet*).


````
usage: nfdump2parquet.py [-h] [-n] [-r] [--debug] [-V] source parquetdir

Convert nfcapd file(s) (produced by the nfdump toolset) to parquet format

positional arguments:
  source           Source nfcapd file or directory containing nfcapd files
  parquetdir       Directory where to store resulting parquet files

options:
  -h, --help       show this help message and exit
  -n, --nohives    Disables hive partitioning (date=YYYYin output parquet directory
  -r, --recursive  recursively searches for nfcapd files if source specifies a directory.
  --debug          show debug output
  -V, --version    print version and exit
````

### coalesce

```
output/
date=2023-02-25
|-- hour=00
|   `-- 2023-02-25-00:00.parquet
|-- hour=01
|   `-- 2023-02-25-01:00.parquet
|-- hour=02
(...)

```


### watch

watch can be used to monitor a directory where nfcapd are stored. Whenever nfcapd rotates to a new file, 
the just finished file will be converted by watch (using nfdump2parquet). Note that watch will not process existing files,
it only detects new files appearing. nfdump2parquet can be used to convert existing files.

_Note that watch monitors a directory recursively, 
which should cover most setups regardless of the sub directory format used by nfcapd._
````
usage: watch.py [-h] [-n] [--debug] [-V] basedir parquetdir

Watches a directory (and its subdirectories) for nfcapd files and converts to parquet

Only files named 'nfcapd.YYYYMMDDHHMM' are picked up, thereby effectively ignoring 
files currently being generated by the nfdump tools.

positional arguments:
  basedir        Base directory to watch for nfdump files
  parquetdir     Base directory where to store parquet files

options:
  -h, --help     show this help message and exit
  -n, --nohives  Disables hive partitioning in output parquet directory
  --debug        show debug output
  -V, --version  print version and exit

````

### parquet_search

parquet_search provides a simple example how to search the resulting parquet files in python 
(using the rather awesome [DuckDB](https://duckdb.org/)).


If you want to directly query the parquet files you can use the [DuckDB CLI](https://duckdb.org/#quickinstall).
Fire it up from the parquet base directory and create a view of the parquet files:
```
CREATE VIEW nfdump AS SELECT * FROM '*/*/*.parquet';
DESCRIBE nfdump;
```
You can use this view exactly as you would a regular table. For example to find the 10 largest flows 
(in number of bytes, input and output combined) you would use:
```
SELECT ts,te,td,sa,sp,da,dp, ibyt+obyt AS tot_byt FROM nfdump ORDER BY tot_byt DESC LIMIT 10;
 ```

If you want to know how long an operation takes you can enable the timer in duckdb with `.timer ON`.

For longer operations you can enable a progress bar with `PRAGMA enable_progress_bar;` 
(disable it with `PRAGMA disable_progress_bar;`)


